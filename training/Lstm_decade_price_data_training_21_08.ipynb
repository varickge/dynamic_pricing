{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_COLUMNS = [ \n",
    "                  'Stellensubart_1', \n",
    "                  'Stellensubart_2',\n",
    "                  'Stellensubart_3', \n",
    "                  'Stellensubart_4', \n",
    "                  'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9',\n",
    "                  'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19',\n",
    "                  'T20', 'T21', 'T22', 'T23', 'T24', 'T25', 'T26', 'T27', 'T28', 'T29',\n",
    "                  'T30', 'T31', 'T32', 'T33', 'T34',\n",
    "                  'TD01', 'TD02', 'TD03', 'TD04', 'TD05', 'TD06', 'TD07', 'TD08', 'TD09',\n",
    "                  'TD10', 'TD11', 'TD12', 'TD13', 'TD14', 'TD15', 'TD16', 'TD17', 'TD18', 'TD19',\n",
    "                  'TD20', 'TD21', 'TD22', 'TD23', 'TD24', 'TD25', 'TD26', 'TD27', 'TD28', 'TD29',\n",
    "                  'TD30', 'TD31', 'TD32', 'TD33', 'TD34',\n",
    "                  'Preis',\n",
    "                  'Beleuchtet', \n",
    "                  'Laenge', \n",
    "                  'Breite', \n",
    "                  'Eigenfl√§che',\n",
    "                  'PPSVACWert',\n",
    "                  'Qid',\n",
    "                  'GJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a013d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data/decade_price_data_combined.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_2018 = pd.read_csv(\"./data/distances/distances_2018_decade_price_data.csv\").set_index(\"Qid1\")\n",
    "distance_2019 = pd.read_csv(\"./data/distances/distances_2019_decade_price_data.csv\").set_index(\"Qid1\")\n",
    "distance_2020 = pd.read_csv(\"./data/distances/distances_2020_decade_price_data.csv\").set_index(\"Qid1\")\n",
    "distance_2021 = pd.read_csv(\"./data/distances/distances_2021_decade_price_data.csv\").set_index(\"Qid1\")\n",
    "distance_2022 = pd.read_csv(\"./data/distances/distances_2022_decade_price_data.csv\").set_index(\"Qid1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955863ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year_count = data.groupby([\"Qid\"])[\"GJ\"].count()\n",
    "# unique_year_counts = data.groupby(\"Qid\")[\"GJ\"].apply(lambda x: x.unique().shape[0])\n",
    "# unique_year_counts = unique_year_counts[(unique_year_counts == 6) & (year_count == 6)]\n",
    "# cleaned_data = data[data.Qid.isin(unique_year_counts.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data.sort_values(by=[\"Qid\", \"GJ\"]).set_index(\"Qid\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.loc[:, CONST_COLUMNS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871a3c8",
   "metadata": {},
   "source": [
    "# Normalize cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.Preis = (cleaned_data.Preis - cleaned_data.Preis.mean()) / cleaned_data.Preis.std()\n",
    "cleaned_data.Laenge = (cleaned_data.Laenge - cleaned_data.Laenge.mean()) / cleaned_data.Laenge.std()\n",
    "cleaned_data.Breite = (cleaned_data.Breite - cleaned_data.Breite.mean()) / cleaned_data.Breite.std()\n",
    "cleaned_data.PPSVACWert = (cleaned_data.PPSVACWert - cleaned_data.PPSVACWert.mean()) / cleaned_data.PPSVACWert.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018 = cleaned_data[cleaned_data.GJ == 2018].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2019 = cleaned_data[cleaned_data.GJ == 2019].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2020 = cleaned_data[cleaned_data.GJ == 2020].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2021 = cleaned_data[cleaned_data.GJ == 2021].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2022 = cleaned_data[cleaned_data.GJ == 2022].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2023 = cleaned_data[cleaned_data.GJ == 2023].drop(columns = [\"Qid\", \"GJ\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023.loc[:, \"T1\":\"T22\"] = data_2023.loc[:, \"T1\":\"T22\"].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4199599",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_qids = cleaned_data.Qid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_train, qid_val = train_test_split(all_valid_qids, test_size=0.05, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c78be7",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d69c95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DataPreprocessor(qids):\n",
    "    \n",
    "    MAX_NEIGH = 10\n",
    "    global data_2018, data_2019, data_2020, data_2021, data_2022, data_2023\n",
    "    global distance_2018, distance_2019, distance_2020, distance_2021, distance_2022\n",
    "    X, x, y  = [], [], []\n",
    "\n",
    "    for idx, qid in enumerate(tqdm(qids)):\n",
    "\n",
    "        ##########################################\n",
    "        neighbours_2018 = distance_2018.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2018, pd.core.series.Series) or (neighbours_2018.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2018 = neighbours_2018[neighbours_2018.Qid2 != qid]\n",
    "        neighbours_2018 = neighbours_2018.Qid2\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2019 = distance_2019.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2019, pd.core.series.Series) or (neighbours_2019.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2019 = neighbours_2019[neighbours_2019.Qid2 != qid]\n",
    "        neighbours_2019 = neighbours_2019.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2020 = distance_2020.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2020, pd.core.series.Series) or (neighbours_2020.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2020 = neighbours_2020[neighbours_2020.Qid2 != qid]\n",
    "        neighbours_2020 = neighbours_2020.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2021 = distance_2021.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2021, pd.core.series.Series) or (neighbours_2021.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2021 = neighbours_2021[neighbours_2021.Qid2 != qid]\n",
    "        neighbours_2021 = neighbours_2021.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2022 = distance_2022.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2022, pd.core.series.Series) or (neighbours_2022.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2022 = neighbours_2022[neighbours_2022.Qid2 != qid]\n",
    "        neighbours_2022 = neighbours_2022.Qid2\n",
    "        #########################################\n",
    "\n",
    "\n",
    "        neighbours_2018_data = torch.from_numpy(data_2018.loc[neighbours_2018.values].values)\n",
    "        neighbours_2019_data = torch.from_numpy(data_2019.loc[neighbours_2019.values].values)\n",
    "        neighbours_2020_data = torch.from_numpy(data_2020.loc[neighbours_2020.values].values)\n",
    "        neighbours_2021_data = torch.from_numpy(data_2021.loc[neighbours_2021.values].values)\n",
    "        neighbours_2022_data = torch.from_numpy(data_2022.loc[neighbours_2022.values].values)\n",
    "\n",
    "\n",
    "        self_data_2018 = torch.from_numpy(data_2018.loc[qid].values)\n",
    "        self_data_2019 = torch.from_numpy(data_2019.loc[qid].values)\n",
    "        self_data_2020 = torch.from_numpy(data_2020.loc[qid].values)\n",
    "        self_data_2021 = torch.from_numpy(data_2021.loc[qid].values)\n",
    "        self_data_2022 = torch.from_numpy(data_2022.loc[qid].values)\n",
    "        self_data_2022 = torch.from_numpy(data_2022.loc[qid].values)\n",
    "        \n",
    "        columns_to_drop = [\"PPSVACWert\", *[f\"T{i}\" for i in range(1, 35)], *[f\"TD{i:02d}\" for i in range(1, 35)]]\n",
    "        self_data_2023 = torch.from_numpy(data_2023.loc[qid].drop(labels=columns_to_drop).values)\n",
    "\n",
    "        # Pad tensors\n",
    "        \n",
    "        neighbours_2018_data_padded = pad(neighbours_2018_data, (0, 0, 0, MAX_NEIGH-neighbours_2018_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2019_data_padded = pad(neighbours_2019_data, (0, 0, 0, MAX_NEIGH-neighbours_2019_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2020_data_padded = pad(neighbours_2020_data, (0, 0, 0, MAX_NEIGH-neighbours_2020_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2021_data_padded = pad(neighbours_2021_data, (0, 0, 0, MAX_NEIGH-neighbours_2021_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2022_data_padded = pad(neighbours_2022_data, (0, 0, 0, MAX_NEIGH-neighbours_2022_data.shape[0]), \"constant\", 0)\n",
    "        \n",
    "        \n",
    "        data_point_2018 = torch.cat([self_data_2018[None], neighbours_2018_data_padded], dim=0)\n",
    "        data_point_2019 = torch.cat([self_data_2019[None], neighbours_2019_data_padded], dim=0)\n",
    "        data_point_2020 = torch.cat([self_data_2020[None], neighbours_2020_data_padded], dim=0)\n",
    "        data_point_2021 = torch.cat([self_data_2021[None], neighbours_2021_data_padded], dim=0)\n",
    "        data_point_2022 = torch.cat([self_data_2022[None], neighbours_2022_data_padded], dim=0)\n",
    "        \n",
    "        \n",
    "        neighbours_features = torch.stack([data_point_2018, \n",
    "                                           data_point_2019,\n",
    "                                           data_point_2020, \n",
    "                                           data_point_2021, \n",
    "                                           data_point_2022,\n",
    "                                          ])\n",
    "\n",
    "        label = torch.tensor(data_2023.loc[qid, \"T1\":\"T22\"].mean())\n",
    "        \n",
    "        X.append(neighbours_features)\n",
    "        x.append(self_data_2023)\n",
    "        y.append(label)\n",
    "        \n",
    "        \n",
    "    X = torch.stack(X, dim=0)\n",
    "    x = torch.stack(x, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "    \n",
    "    \n",
    "    return X, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train data\n",
    "X_train, x_train, y_train = DataPreprocessor(qid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13987ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_train, \"./data/proof_of_concept/X_train_lstm.pt\")\n",
    "torch.save(x_train, \"./data/proof_of_concept/x_train_lstm.pt\")\n",
    "torch.save(y_train, \"./data/proof_of_concept/y_train_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb275eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading val data\n",
    "X_val, x_val, y_val = DataPreprocessor(qid_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_val, \"./data/proof_of_concept/X_val_lstm.pt\")\n",
    "torch.save(x_val, \"./data/proof_of_concept/x_val_lstm.pt\")\n",
    "torch.save(y_val, \"./data/proof_of_concept/y_val_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53758c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading test data\n",
    "X_test, x_test, y_test = DataPreprocessor([9860, 9673, 9855])\n",
    "\n",
    "#error with 9333\n",
    "\n",
    "torch.save(X_test, \"./data/proof_of_concept/X_test_lstm.pt\")\n",
    "torch.save(x_test, \"./data/proof_of_concept/x_test_lstm.pt\")\n",
    "torch.save(y_test, \"./data/proof_of_concept/y_test_lstm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4355e",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37212e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel_1(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "\n",
    "        super(LSTMModel_1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bias=True, dropout=0.0)\n",
    "        \n",
    "        self.fc_1 = nn.Sequential(nn.Linear(11, 1),\n",
    "        )\n",
    "        \n",
    "        self.fc_2 = nn.Sequential(nn.Linear(hidden_size // 1, hidden_size // 2),\n",
    "                                  nn.LeakyReLU(),\n",
    "                                  nn.BatchNorm1d(hidden_size // 2),\n",
    "                                  nn.Dropout1d(0.0),\n",
    "                                  nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "                                  nn.LeakyReLU(),\n",
    "                                  nn.Dropout1d(0.0),\n",
    "                                  nn.Linear(hidden_size // 4, hidden_size // 8),\n",
    "                                  nn.LeakyReLU(),\n",
    "                                  nn.Dropout1d(0.0),\n",
    "                                  nn.Linear(hidden_size // 8, output_size),\n",
    "        )\n",
    "        \n",
    "        self.fc_b = nn.Linear(input_size - 69, input_size)\n",
    "        \n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, x_2023):\n",
    "        \n",
    "        out = self.fc_1(x)\n",
    "        out = out[..., 0]\n",
    "        \n",
    "        x_2023 = self.fc_b(x_2023)[:, None, ...]\n",
    "        out = torch.cat([out, x_2023], dim=1)\n",
    "        \n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, ...]\n",
    "        \n",
    "        out = self.fc_2(out)\n",
    "        \n",
    "        out = self.sigmoid(out)\n",
    "                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_X, path_x, path_y, p=0.1, smooth_labels=False):\n",
    "        \n",
    "        self.data_X = torch.load(path_X)\n",
    "        self.data_X = self.data_X.permute(0, 1, 3, 2)\n",
    "        \n",
    "        self.data_x = torch.load(path_x)\n",
    "        self.data_y = torch.load(path_y)\n",
    "    \n",
    "        self.p = p\n",
    "        \n",
    "        if smooth_labels is True:\n",
    "            self.smoothing_eps = lambda : ((torch.rand(1) -0.5) /(2 * 34)).item()\n",
    "        else:\n",
    "            self.smoothing_eps = lambda : 0\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return self.__transform(self.data_X[index].clone(), self.data_x[index].clone(), self.data_y[index].clone())\n",
    "        else:\n",
    "            return self.data_X[index], self.data_x[index], self.data_y[index] + self.smoothing_eps()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "    \n",
    "    def __transform(self, item_X, item_x, item_y, k=2):\n",
    "        \n",
    "        max_price = max(item_X[:, 0, -6])\n",
    "        item_x[-5] = k * max_price\n",
    "        item_y = item_y * 0\n",
    "        \n",
    "        return item_X, item_x, item_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851d1e4",
   "metadata": {},
   "source": [
    "# Defining hyperparametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead06f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel_1(78, 512, 5, 1).to(DEVICE)\n",
    "criterion = nn.MSELoss(reduce=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"./data/proof_of_concept/X_train_lstm.pt\", \n",
    "                              \"./data/proof_of_concept/x_train_lstm.pt\",\n",
    "                              \"./data/proof_of_concept/y_train_lstm.pt\")\n",
    "\n",
    "val_dataset = CustomDataset(  \"./data/proof_of_concept/X_val_lstm.pt\", \n",
    "                              \"./data/proof_of_concept/x_val_lstm.pt\",\n",
    "                              \"./data/proof_of_concept/y_val_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b630bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5576e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1/20\n",
    "bin_edges = torch.arange(0, 1+step, step)\n",
    "bin_indices = torch.bucketize(train_dataset.data_y, bin_edges)\n",
    "_, counts = torch.unique(bin_indices, return_counts=True)\n",
    "loss_weights = torch.softmax(-counts / counts.sum() *  10, dim=-1).to(DEVICE)\n",
    "plt.figure(figsize=(2, 2)); plt.plot(loss_weights.cpu()); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_loss(labels, loss, loss_weights, x):\n",
    "    \n",
    "    grad,  = torch.autograd.grad(loss.mean(), x, create_graph=True, retain_graph=True)\n",
    "    return grad.pow(2).mean() + loss.mean()\n",
    "    \n",
    "    global bin_edges\n",
    "    \n",
    "    bin_indices = torch.bucketize(labels, bin_edges.to(DEVICE))    \n",
    "    return (loss_weights[bin_indices] * loss).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c527e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "history = []\n",
    "val_min_loss = None\n",
    "model_name = input(\"Input proper model name:\\t\")\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=4, verbose=True)\n",
    "\n",
    "with torch.backends.cudnn.flags(enabled=False):\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_running_loss = 0.0\n",
    "        model.train()\n",
    "            \n",
    "        for batch in tqdm(train_dataloader, position=0, leave=True):\n",
    "            # Every data instance is an input + label pair\n",
    "            X_train1 = batch[0].to(torch.float32).to(DEVICE)\n",
    "            X_train1.requires_grad = True\n",
    "            x_train1 = batch[1].to(torch.float32).to(DEVICE)\n",
    "            y_train1 = batch[2].to(torch.float32).to(DEVICE)\n",
    "            outputs = model(X_train1, x_train1)\n",
    "            loss = criterion(outputs, y_train1.view(-1, 1))\n",
    "            loss = compute_weighted_loss(y_train1.view(-1, 1), loss, loss_weights, X_train1)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            optimizer.zero_grad()\n",
    "            train_running_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_running_loss = 0.\n",
    "        \n",
    "\n",
    "        for batch in tqdm(val_dataloader, position=0, leave=True):\n",
    "\n",
    "            # Every data instance is an input + label pair\n",
    "            X_val1 = batch[0].to(torch.float32).to(DEVICE)\n",
    "            X_val1.requires_grad = True\n",
    "            x_val1 = batch[1].to(torch.float32).to(DEVICE)\n",
    "            y_val1 = batch[2].to(torch.float32).to(DEVICE)\n",
    "            outputs = model(X_val1, x_val1)\n",
    "            loss = criterion(outputs, y_val1.view(-1, 1))\n",
    "            loss = compute_weighted_loss(y_val1.view(-1, 1), loss, loss_weights, X_val1)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "        mean_train_loss = train_running_loss/len(train_dataloader)\n",
    "        mean_val_loss = val_running_loss/len(val_dataloader)\n",
    "        \n",
    "        # scheduler.step(mean_val_loss)\n",
    "        \n",
    "        if val_min_loss is None:\n",
    "            val_min_loss = mean_val_loss\n",
    "        elif mean_val_loss < val_min_loss:\n",
    "                val_min_loss = mean_val_loss\n",
    "                torch.save(model.state_dict(), f'./models/{model_name}.pth')\n",
    "        \n",
    "        history.append([mean_train_loss, mean_val_loss])\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\\nTrain Loss: {round(mean_train_loss, 4)}\\nVal Loss: {round(mean_val_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.legend([\"train\", \"val\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3988ecf",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_val = val_dataset.data_X\n",
    "x_val = val_dataset.data_x\n",
    "y_val = val_dataset.data_y\n",
    "preds = model(X_val.to(torch.float32).to(DEVICE), x_val.to(torch.float32).to(DEVICE)).detach().cpu().numpy()\n",
    "ground_truth = (y_val).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9305c07",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18667580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.abs(preds[..., 0] - ground_truth).mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c5a1a",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ce634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot([ground_truth, preds[..., 0]])\n",
    "plt.legend([\"Ground truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e58f2a",
   "metadata": {},
   "source": [
    "## Scatter plot of ground_truth and preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c65d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagonal line for y = x\n",
    "x_values = np.linspace(min(preds[..., 0].min(), ground_truth.min()), max(preds[..., 0].max(), ground_truth.max()), 100)\n",
    "y_values = x_values\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=ground_truth, y=preds[..., 0])\n",
    "\n",
    "# Plot the diagonal line\n",
    "plt.plot(x_values, y_values, color='red', linestyle='dashed', label='y = x')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Ground Truth')\n",
    "\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Scatter plot - Predictions vs. Ground Truth')\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
