{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03f8955",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from tst import Transformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_COLUMNS = [ \n",
    "                  'Stellensubart_1', \n",
    "                  'Stellensubart_2',\n",
    "                  'Stellensubart_3', \n",
    "                  'Stellensubart_4', \n",
    "                  'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9',\n",
    "                  'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19',\n",
    "                  'T20', 'T21', 'T22', 'T23', 'T24', 'T25', 'T26', 'T27', 'T28', 'T29',\n",
    "                  'T30', 'T31', 'T32', 'T33', 'T34',\n",
    "                  'Preis',\n",
    "                  'Beleuchtet', \n",
    "                  'Laenge', \n",
    "                  'Breite', \n",
    "                  'Eigenfl√§che',\n",
    "                  'PPSVACWert',\n",
    "                  'Qid',\n",
    "                  'GJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d93dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data/df_gesamt_15_08_prepocessed_einworner_added.pkl\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_2018 = pd.read_csv(\"./data/distances/distances_2018_truncated.csv\").set_index(\"Qid1\")\n",
    "distance_2019 = pd.read_csv(\"./data/distances/distances_2019_truncated.csv\").set_index(\"Qid1\")\n",
    "distance_2020 = pd.read_csv(\"./data/distances/distances_2020_truncated.csv\").set_index(\"Qid1\")\n",
    "distance_2021 = pd.read_csv(\"./data/distances/distances_2021_truncated.csv\").set_index(\"Qid1\")\n",
    "distance_2022 = pd.read_csv(\"./data/distances/distances_2022_truncated.csv\").set_index(\"Qid1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c314b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = data.groupby([\"Qid\"])[\"GJ\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data[data.Qid.isin(year_count[year_count == 6].index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a369c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.sort_values(by=[\"Qid\", \"GJ\"]).set_index(\"Qid\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a679d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.loc[:, CONST_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.Laenge = (cleaned_data.Laenge - cleaned_data.Laenge.mean()) / cleaned_data.Laenge.std()\n",
    "cleaned_data.Breite = (cleaned_data.Breite - cleaned_data.Breite.mean()) / cleaned_data.Breite.std()\n",
    "cleaned_data.PPSVACWert = (cleaned_data.PPSVACWert - cleaned_data.PPSVACWert.mean()) / cleaned_data.PPSVACWert.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018 = cleaned_data[cleaned_data.GJ == 2018].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2019 = cleaned_data[cleaned_data.GJ == 2019].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2020 = cleaned_data[cleaned_data.GJ == 2020].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2021 = cleaned_data[cleaned_data.GJ == 2021].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2022 = cleaned_data[cleaned_data.GJ == 2022].drop(columns = [\"Qid\", \"GJ\"]).copy()\n",
    "data_2023 = cleaned_data[cleaned_data.GJ == 2023].drop(columns = [\"Qid\", \"GJ\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4814f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2023.loc[:, \"T1\":\"T22\"] = data_2023.loc[:, \"T1\":\"T22\"].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_qids = cleaned_data.Qid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f93944",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_train, qid_val = train_test_split(all_valid_qids, test_size=0.05, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocessor(qids):\n",
    "    \n",
    "    MAX_NEIGH = 10\n",
    "    global data_2018, data_2019, data_2020, data_2021, data_2022, data_2023\n",
    "    global distance_2018, distance_2019, distance_2020, distance_2021, distance_2022\n",
    "    X, x, y  = [], [], []\n",
    "\n",
    "    for idx, qid in enumerate(tqdm(qids)):\n",
    "\n",
    "        ##########################################\n",
    "        neighbours_2018 = distance_2018.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2018, pd.core.series.Series) or (neighbours_2018.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2018 = neighbours_2018[neighbours_2018.Qid2 != qid]\n",
    "        neighbours_2018 = neighbours_2018.Qid2\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2019 = distance_2019.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2019, pd.core.series.Series) or (neighbours_2019.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2019 = neighbours_2019[neighbours_2019.Qid2 != qid]\n",
    "        neighbours_2019 = neighbours_2019.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2020 = distance_2020.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2020, pd.core.series.Series) or (neighbours_2020.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2020 = neighbours_2020[neighbours_2020.Qid2 != qid]\n",
    "        neighbours_2020 = neighbours_2020.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2021 = distance_2021.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2021, pd.core.series.Series) or (neighbours_2021.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2021 = neighbours_2021[neighbours_2021.Qid2 != qid]\n",
    "        neighbours_2021 = neighbours_2021.Qid2\n",
    "\n",
    "        #########################################\n",
    "        neighbours_2022 = distance_2022.loc[qid]\n",
    "\n",
    "        if isinstance(neighbours_2022, pd.core.series.Series) or (neighbours_2022.shape[0] <= 1):\n",
    "            continue\n",
    "\n",
    "        neighbours_2022 = neighbours_2022[neighbours_2022.Qid2 != qid]\n",
    "        neighbours_2022 = neighbours_2022.Qid2\n",
    "        #########################################\n",
    "\n",
    "\n",
    "        neighbours_2018_data = torch.from_numpy(data_2018.loc[neighbours_2018.values].values)\n",
    "        neighbours_2019_data = torch.from_numpy(data_2019.loc[neighbours_2019.values].values)\n",
    "        neighbours_2020_data = torch.from_numpy(data_2020.loc[neighbours_2020.values].values)\n",
    "        neighbours_2021_data = torch.from_numpy(data_2021.loc[neighbours_2021.values].values)\n",
    "        neighbours_2022_data = torch.from_numpy(data_2022.loc[neighbours_2022.values].values)\n",
    "\n",
    "\n",
    "        self_data_2018 = torch.from_numpy(data_2018.loc[qid].values)\n",
    "        self_data_2019 = torch.from_numpy(data_2019.loc[qid].values)\n",
    "        self_data_2020 = torch.from_numpy(data_2020.loc[qid].values)\n",
    "        self_data_2021 = torch.from_numpy(data_2021.loc[qid].values)\n",
    "        self_data_2022 = torch.from_numpy(data_2022.loc[qid].values)\n",
    "        self_data_2022 = torch.from_numpy(data_2022.loc[qid].values)\n",
    "        self_data_2023 = torch.from_numpy(data_2023.loc[qid].drop(labels=[\"PPSVACWert\", *[f\"T{i}\" for i in range(1, 35)]]).values)\n",
    "\n",
    "        # Pad tensors\n",
    "        \n",
    "        neighbours_2018_data_padded = pad(neighbours_2018_data, (0, 0, 0, MAX_NEIGH-neighbours_2018_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2019_data_padded = pad(neighbours_2019_data, (0, 0, 0, MAX_NEIGH-neighbours_2019_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2020_data_padded = pad(neighbours_2020_data, (0, 0, 0, MAX_NEIGH-neighbours_2020_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2021_data_padded = pad(neighbours_2021_data, (0, 0, 0, MAX_NEIGH-neighbours_2021_data.shape[0]), \"constant\", 0)\n",
    "        neighbours_2022_data_padded = pad(neighbours_2022_data, (0, 0, 0, MAX_NEIGH-neighbours_2022_data.shape[0]), \"constant\", 0)\n",
    "        \n",
    "        \n",
    "        data_point_2018 = torch.cat([self_data_2018[None], neighbours_2018_data_padded], dim=0)\n",
    "        data_point_2019 = torch.cat([self_data_2019[None], neighbours_2019_data_padded], dim=0)\n",
    "        data_point_2020 = torch.cat([self_data_2020[None], neighbours_2020_data_padded], dim=0)\n",
    "        data_point_2021 = torch.cat([self_data_2021[None], neighbours_2021_data_padded], dim=0)\n",
    "        data_point_2022 = torch.cat([self_data_2022[None], neighbours_2022_data_padded], dim=0)\n",
    "        \n",
    "        \n",
    "        neighbours_features = torch.stack([data_point_2018, \n",
    "                                           data_point_2019,\n",
    "                                           data_point_2020, \n",
    "                                           data_point_2021, \n",
    "                                           data_point_2022,\n",
    "                                          ])\n",
    "\n",
    "        label = torch.tensor(data_2023.loc[qid, \"T1\":\"T22\"].mean())\n",
    "        \n",
    "        X.append(neighbours_features)\n",
    "        x.append(self_data_2023)\n",
    "        y.append(label)\n",
    "        \n",
    "    X = torch.stack(X, dim=0)\n",
    "    x = torch.stack(x, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "    \n",
    "    \n",
    "    return X, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1356ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_X, path_x, path_y, p=0.0):\n",
    "        \n",
    "        self.data_X = torch.load(path_X)\n",
    "        SH = self.data_X.shape\n",
    "        self.data_X = self.data_X.reshape(SH[0], SH[2], -1)\n",
    "        \n",
    "        self.data_x = torch.load(path_x)\n",
    "        self.data_y = torch.load(path_y)\n",
    "    \n",
    "        self.p = p\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return self.__transform(self.data_X[index].clone(), self.data_x[index].clone(), self.data_y[index].clone())\n",
    "        else:\n",
    "            return self.data_X[index], self.data_x[index], self.data_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "    \n",
    "    def __transform(self, item_X, item_x, item_y, k=2):\n",
    "        \n",
    "        max_price = max(item_X[:, 0, -6])\n",
    "        item_x[-5] = k * max_price\n",
    "        item_y = item_y * 0\n",
    "        \n",
    "        return item_X, item_x, item_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_train, y_train = DataPreprocessor(qid_train)\n",
    "X_val, x_val, y_val = DataPreprocessor(qid_val)\n",
    "torch.save(X_train, \"./data/proof_of_concept/X_train.pt\")\n",
    "torch.save(x_train, \"./data/proof_of_concept/x_train.pt\")\n",
    "torch.save(y_train, \"./data/proof_of_concept/y_train.pt\")\n",
    "torch.save(X_val, \"./data/proof_of_concept/X_val.pt\")\n",
    "torch.save(x_val, \"./data/proof_of_concept/x_val.pt\")\n",
    "torch.save(y_val, \"./data/proof_of_concept/y_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, x_test, y_test = DataPreprocessor([9860.0, 9673.0, 9855.0])\n",
    "torch.save(X_test, \"./data/proof_of_concept/X_test.pt\")\n",
    "torch.save(x_test, \"./data/proof_of_concept/x_test.pt\")\n",
    "torch.save(y_test, \"./data/proof_of_concept/y_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9224af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_input=220, d_model=1024, d_output=16, q=8, v=8, h=20, N=20, chunk_mode=None)\n",
    "        self.target_year_linear = nn.Linear(9, 16)\n",
    "        self.intermediate_linear = nn.Linear(192, 32)\n",
    "        self.prediction_head = nn.Linear(32, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(32)\n",
    "    \n",
    "    def forward(self, X, x):\n",
    "        \n",
    "        transformer_features = self.transformer(X)\n",
    "        \n",
    "        x_features = self.target_year_linear(x)\n",
    "                \n",
    "        transformer_features = transformer_features.reshape(transformer_features.shape[0], -1)\n",
    "        \n",
    "        joint_features = torch.cat([transformer_features, x_features], dim=-1)\n",
    "        \n",
    "        joint_features = self.activation(self.batchnorm(self.intermediate_linear(joint_features)))\n",
    "        \n",
    "        prediction = self.prediction_head(joint_features)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device=DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"./data/proof_of_concept/X_train.pt\", \n",
    "                              \"./data/proof_of_concept/x_train.pt\",\n",
    "                              \"./data/proof_of_concept/y_train.pt\")\n",
    "\n",
    "val_dataset = CustomDataset(  \"./data/proof_of_concept/X_val.pt\", \n",
    "                              \"./data/proof_of_concept/x_val.pt\",\n",
    "                              \"./data/proof_of_concept/y_val.pt\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "history = []\n",
    "val_min_loss = None\n",
    "model_name = input(\"Input proper model name:\\t\")\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              base_lr=LEARNING_RATE, \n",
    "                                              max_lr=0.003, \n",
    "                                              cycle_momentum=True,\n",
    "                                              mode='triangular2',\n",
    "                                              verbose=False)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        # Every data instance is an input + label pair\n",
    "        X_train1 = batch[0].to(torch.float32).to(DEVICE)\n",
    "        x_train1 = batch[1].to(torch.float32).to(DEVICE)\n",
    "        y_train1 = batch[2].to(torch.float32).to(DEVICE)\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(X_train1, x_train1)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, y_train1.view(-1, 1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Gather data and report\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
    "\n",
    "            # Every data instance is an input + label pair\n",
    "            X_val1 = batch[0].to(torch.float32).to(DEVICE)\n",
    "            x_val1 = batch[1].to(torch.float32).to(DEVICE)\n",
    "            y_val1 = batch[2].to(torch.float32).to(DEVICE)\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(X_val1, x_val1)\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs, y_val1.view(-1, 1))\n",
    "            \n",
    "            # Gather data and report\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    mean_train_loss = train_running_loss/len(train_dataloader)\n",
    "    mean_val_loss = val_running_loss/len(val_dataloader)\n",
    "    \n",
    "    if val_min_loss is None:\n",
    "        val_min_loss = mean_val_loss\n",
    "    else:\n",
    "        if mean_val_loss < val_min_loss:\n",
    "            val_min_loss = mean_val_loss\n",
    "            torch.save(model.state_dict(), f'./models/{model_name}.pth')\n",
    "    \n",
    "    \n",
    "    history.append([mean_train_loss, mean_val_loss])\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\\nTrain Loss: {round(mean_train_loss, 4)}\\nVal Loss: {round(mean_val_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./models/train_transformer_21_08.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc534e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cpu()\n",
    "X_val = val_dataset.data_X\n",
    "x_val = val_dataset.data_x\n",
    "y_val = val_dataset.data_y\n",
    "preds = model(X_val.to(torch.float32), x_val.to(torch.float32)).detach().cpu().numpy()\n",
    "ground_truth = (y_val).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ebf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ground_truth, c=\"red\")\n",
    "sns.kdeplot(preds)\n",
    "plt.legend([\"ground_truth\", \"prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagonal line for y = x\n",
    "x_values = np.linspace(min(preds[..., 0].min(), ground_truth.min()), max(preds[..., 0].max(), ground_truth.max()), 100)\n",
    "y_values = x_values\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=ground_truth, y=preds[..., 0])\n",
    "\n",
    "# Plot the diagonal line\n",
    "plt.plot(x_values, y_values, color='red', linestyle='dashed', label='y = x')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Ground Truth')\n",
    "\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Scatter plot - Predictions vs. Ground Truth')\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
